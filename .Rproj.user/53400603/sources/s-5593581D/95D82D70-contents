demo_table<-read.csv(file="demo.csv", check.names = F, stringsAsFactors = F)

library(tidyverse)
library(jsonlite)
?fromJSON()
jsoncars <- toJSON(mtcars, pretty=TRUE)
cat(jsoncars)

# SUBSETTING, SELECTIONG DATA ----
#You can also use bracket notation to select data from two-dimensional 
#data structures, such as matrices, data frames, and tibbles.
demo_table2<- fromJSON(txt='demo.json')
demo_table[3,"Year"]
demo_table[3,3]
demo_table$Vehicle_Class[3]






# (FILTERING) Select Data with Logic ----
#One of the most common ways to filter and subset a dataset in R 
#is to use bracket notation. To use bracket notation to filter a data frame, 
#we can supply a logical statement to assert our row and columns.

  #if we want to filter our used car data demo_table2 so that we only 
  #have rows where the vehicle price is greater than $10,000, 
  #we would use the following statement:

  filter_table<-demo_table2 [demo_table2$price>10000]




# Subset Data in R ----

#Another method to filter and subset data frames in R is to use the function
#subset().

#The subset() function uses a few arguments to subset and filter a 
#two-dimensional R data structure:
  
    #x == is the matrix, data frame, or tibble we wish to subset.
    #select ==  logical statements that determine which rows to keep.
    #subset == logical statement, or list of column names to select from a 
                #data 

    #For example, if we want to create a more elaborate filtered dataset 
    #from our used car data demo_table2 where price > 10000, drive == 4wd, 
    #and "clean" %in% title_status, we would use the following statement:

    filter_table2<- subset(demo_table2, price>10000 & 
                                        drive =="4wd" &
                                        "clean" %in% title_status)

    filter_table3 <- demo_table2[("clean" %in% demo_table2$title_status) & 
                                 (demo_table2$price > 10000) & 
                                 (demo_table2$drive == "4wd"),]

    

    
# Sample Data in R ----
   
  #looking at data within a specific timeframe we'll want to randomly 
  #sample our larger data to reduce bias. In these cases, 
  #we can use the built-in function sample().
    
    #The sample() function uses a few arguments to create a sampled 
    #vector from a larger vector:
      
      # x  == is the larger vector to select from.
      # size == is the number of sample data points to select from x
      # replace == is a flag to tell whether or not it is okay to select the 
      # same values
    
       sample(c("cow", "deer", "pig", "chicken", "duck", "sheep", "dog"), 4)
    
    
      #capture the number of rows in demo_table in a variable. 
    
      #The nrow() function counts the number of rows in a dataframe.
    
      num_rows <- 1:nrow(demo_table)
   
      demo_table[sample(1:nrow(demo_table), 3),]
      

      
      
#Transform, Group, and Reshape Data Using the Tidyverse Package ----
    dplyr allows the user to chain together functions in a single statement 
    using their own pipe operator (%>%).  
    
    To transform a data frame and include new calculated data columns, 
    well use the mutate() function.
      
  #(if we want to use our coworker vehicle data from the demo_table and add 
  #a column for the mileage per year, as well as 
  #label all vehicles as active, we would use the following statement:)
      # add columns to original data frame
      
      library(tidyverse)
      demo_table <- demo_table %>% mutate(Mileage_per_Year=Total_Miles/(2020-Year),IsActive=TRUE)
      

      
      
      
      
#GROUP BY  ----
      
      #The group_by() function tells dplyr which factor 
      #(or list of factors in order) to group our data frame by.
      
      #create summary table
      
      The summarize() function takes an additional argument, .groups. 
      This allows you to control the the grouping of the result. 
      The four possible values are:
      
      .groups = "drop_last" drops the last grouping level (default)
      .groups = "drop" drops all grouping levels and returns a tibble
      .groups = "keep" preserves the grouping of the input
      .groups = "rowwise" turns each row into its own group
      
summarize_demo <- 
        
    demo_table2 %>% group_by(condition) %>% summarize(Mean_Mileage=mean(odometer), .groups = 'keep') 
      
      #Our simplest summary functions will use statistics summary functions 
      #such as 
      mean(), 
      median(), 
      sd(), 
      min(), 
      max(), and 
      n() (used to calculate the number of rows in each category).
      
      
#RESHAPE DATA ----
      the tidyr library from the tidyverse has the 
      
      gather() and spread() functions  to help reshape our data. 
      
      #The gather() function is used to transform a wide dataset into a 
      
      long dataset.
      
      To properly reshape an R data frame, the gather() function requires a 
      few arguments:
        
      data == is the data frame we wish to reshape
      key  == is the name of the variable column that the original, 
              wider data frame will collapse into.
      value== Is the name of the new value column derived from the original 
              data frame results.
      ...  == represents a list of columns to collapse into the key column
      
      
demo_table3 <- read.csv('demo2.csv', check.names = F, stringsAsFactors = F) 

long_table <- gather(demo_table3, key="Metric",value="Score",buying_price:popularity)

long_table <- demo_table3 %>% gather(key="Metric",value="Score",buying_price:popularity)

 wide_table <- long_table %>% spread(key="Metric",value="Score")
 
       #Alternatively, if we have data that was collected or obtained in a long format, we can use tidyrs 
       spread() function to spread out a variable column of multiple measurements into columns 
       for each variable.
       
       the spread() function requires a few arguments:
         
       data == is the data frame we wish to reshape
       key == is the name of the variable column that we wish to spread out.
       value == is the name of the value column that we wish to fill in our new variable columns with.
       fill == is an optional argument that will set any empty rows in a new variable column with the fill value.
      
 wide_table <- long_table %>% spread(key="Metric",value="Score")
 
 all.equal(demo_table3,wide_table)
 

 plt <- ggplot(mpg,aes(x=class)) #import dataset into ggplot2
 plt + geom_bar() #plot a bar plot
 
 mpg_summary <- mpg %>% group_by(manufacturer) %>% summarize(Vehicle_Count=n(), .groups = 'keep') #create summary table
 plt <- ggplot(mpg_summary,aes(x=manufacturer,y=Vehicle_Count)) 
 plt + geom_col() #plot a bar plot
 
 
 #15.3.1 Introduction to ggplot2----
 Introduction to ggplot2 
 
 well learn about the ggplot library components and how to implement our 
 basic plots, such as bar, line, and scatter plot.
 
 All figures in ggplot2 are created using the same three components:
   
  1-ggplot function—tells ggplot2 what variables to use
  2-geom function—tells ggplot2 what plots to generate
  3-formatting or theme functions—tells ggplot2 how to customize the plot
  
  
  1-we need to declare our input data and variables using the ggplot() function.
  
    ?ggplot()
  
          The ggplot() function only requires two arguments to declare 
          the input data:
    
              data     ==Response area is our input data frame.
              mapping  ==ses the aes() aesthetic function to tell ggplot()
                          what variables are assigned to the x (independent) 
                          and y (dependent) variables.
                          
                          There are a number of optional aes() arguments to 
                          assign such as color, fill, shape, and size to 
                          customize the plots
                          
  **********************************************************************
  #15.3.2 Bar Plot in ggplot2----
    Build a Bar Plot in ggplot2
                          
                            
The mpg dataset contains fuel ecnmy data from the EPA for vehicles manufactured 
between 1999 and 2008. The mpg dataset is built into R and is used 
throughout R documentation 
                            
  head(mpg)
  
  #plot a bar plot with single variable
  
  plt <- ggplot(mpg, aes(x=class))  #import dataset into ggplot2
  plt + geom_bar() #plot a bar plot
  
 
  ?geom_bar()
  
  #plot a bar plot with two variables
  
  Another use for bar plots is to compare and contrast categorical results. 
  For example, if we want to compare the number of vehicles from each manufacturer 
  in the dataset, we can use dplyrs summarize() function to summarize the data, 
  and ggplot2s geom_col() to visualize the results:
  
  #create summary table
  
mpg_summary <- mpg %>% group_by(manufacturer) %>% summarize(Vehicle_Count=n(), .groups = 'keep')
  
plt <- ggplot(mpg_summary,aes(x=manufacturer,y=Vehicle_Count)) #import dataset into ggplot2
plt + geom_col() #plot a bar plot

*******************************************************************************
  
Functionally, both geom_bar() and geom_col()create bar plots; however, the two 
methods assume different inputs. 

    geom_bar() expects one variable and generates frequency data, and 
    geom_col()expects two variables where we provide the size of each ctgrys bar
    

    #15.3.3 Formatting Functions----
    Add Formatting Functions
    
    To change the titles of our x-axis and y-axis, we can use the 
    xlab()and ylab()functions, respectively:
      
    we set the angle argument of our 
    
    element_text() function to 45 degrees and 
    hjust argument to 1. 
    
    if we want to adjust our y-axis labels, we would do so by using the 
    axis.text.y argument of the theme() function
      
    #plot bar plot with labels
      
plt+geom_col()+xlab("Manufacturing Company")+ylab("Number of Vehicles in Dataset")
    
    #rotate the x-axis label 45 degrees

plt + geom_col()+ xlab("Manufacturing Company")+ ylab("Number of Vehicles in Dataset") + 
   theme(axis.text.x=element_text(angle=45,hjust=1))

     #15.3.4 Line Plot in ggplot2----
  Build a Line Plot in ggplot2

  When creating the ggplot object for our line data, we need to set the 
  categorical variable to the x value and 
  continuous  variable to the y value within our aes() function
  
  ***********************
  For exp, if we want to compare the differences in avg highway fuel economy (hwy) 
  of Toyota vehicles as a function of the different cylinder sizes (cyl), 
  our R code would look like the following:
    
    #create summary table
    mpg_summary <- subset(mpg,manufacturer=="toyota") %>% group_by(cyl) %>% summarize(Mean_Hwy=mean(hwy), .groups = 'keep')

    #import dataset into ggplot2
    plt <- ggplot(mpg_summary,aes(x=cyl,y=Mean_Hwy))
    
    #generate our line plot using geom_line():
    plt + geom_line()
    
    default x-axis misrepresents the data because there are no five- and 
    seven-cylinder vehicles.
    To adjust the x-axis and y-axis tick values, well use the 
    scale_x_discrete() and 
    scale_y_continuous() functions:
      
    #add line plot with labels
    plt + geom_line() + scale_x_discrete(limits=c(4,6,8)) + scale_y_continuous(breaks = c(15:30))
    
    plt <- ggplot(mpg,aes(x=displ,y=cty))
    
    #add scatter plot with labels
    plt + geom_point() + xlab("Engine Size (L)") + ylab("City Fuel-Efficiency (MPG)")
    
  There are a number of customizing aesthetics we can add to our aes() function 
  to change our scatter plot data points, such as:
      
    alpha changes the transparency of each data point
    color changes the color of each data point
    shape changes the shape of each data point
    size changes the size of each data point
    
    #import dataset into ggplot2
    ggplot(mpg,aes(x=displ,y=cty,color=class))
    
    #add scatter plot with labels
    plt + geom_point() + labs(x="Engine Size (L)", y="City Fuel-Efficiency (MPG)", color="Vehicle Class")
    
    #add scatter plot with multiple aesthetics
    plt <- ggplot(mpg,aes(x=displ,y=cty,color=class,shape=drv)) #import dataset into ggplot2
    plt + geom_point() + labs(x="Engine Size (L)", y="City Fuel-Efficiency (MPG)", color="Vehicle Class",shape="Type of Drive")
  
    
      #15.3.5 Advanced Boxplots in ggplot2----
    Create Advanced Boxplots in ggplot2
    
when performing statistical analysis, we may want to visualize summary 
statistics using boxplots, or unpack the relationship across multiple variables 
using a heatmap.

ggplot2 has functions such as 
    geom_boxplot() and 
    geom_tile()
    
    #generate a boxplot to visualize the highway fuel efficiency of our mpg dataset,
    
        #import dataset into ggplot2
        plt<- ggplot(mpg,aes(y=hwy))
        
        #add boxplot
        plt + geom_boxplot()
        
    # create a set of boxplots that compares highway fuel efficiency for each 
    #car manufacturer,
        #import dataset into ggplot2
        plt<- ggplot(mpg,aes(x=manufacturer,y=hwy))
        
        #add boxplot and rotate x-axis labels 45 degrees
        plt + geom_boxplot() + theme(axis.text.x=element_text(angle=45,hjust=1))
        
        plt + geom_boxplot(fill= "green", color= "blue")
        
        # By default, outlier points match the colour of the box. Use
        # outlier.colour to override
        
        plt + geom_boxplot(outlier.colour = "red", outlier.shape = 1)
        
        # Boxplots are automatically dodged when any aesthetic is a factor
        plt + geom_boxplot(aes(colour = drv))
        
  
        #15.3.6  Create Heatmap Plots ----
        Create Heatmap Plots
        
        
       Heatmap plots help visualize the relationship between one continuous 
       numerical variable and two other variables (categorical or numerical).
       
       ********************************
    if we want to visualize the average highway fuel efficiency across the type of 
       vehicle class from 1999 to 2008,
       
       #create summary table
       mpg_summary<- mpg %>% group_by(class,year) %>% summarise(Mean_Hwy= mean(hwy), .groups = 'keep')
       
       plt <- ggplot(mpg_summary, aes(x=class,y=factor(year),fill=Mean_Hwy))
    
       #create heatmap with labels
       plt + geom_tile() + labs(x="Vehicle Class",y="Vehicle Year",fill="Mean Highway (MPG)") 
       
       #formatted with 45 degree
       plt + geom_tile() + labs(x="Vehicle Class",y="Vehicle Year",fill="Mean Highway (MPG)") + theme(axis.text.x=element_text(angle=45,hjust=1))
       
      #if we want to look at the difference in average highway fuel efficiency
      #across each vehicle model from 1999 to 2008, 
       
       #create summary table
       mpg_summary <- mpg %>% group_by(model,year) %>% summarize(Mean_Hwy=mean(hwy), .groups = 'keep')
       
       plt <- ggplot(mpg_summary, aes(x=model,y=factor(year),fill=Mean_Hwy)) #import dataset into ggplot2
    
       #add heatmap with labels >  and #rotate x-axis labels 90 degrees  
       plt + geom_tile() + labs(x="Model",y="Vehicle Year",fill="Mean Highway (MPG)") + theme(axis.text.x = element_text(angle=90,hjust=1,vjust=.5))
       
          #15.3.7 Add Layers to Plots ----
      
   Add Layers to Plots
  
  FACETING; process of separating out plots for each level is known as 
  faceting in ggplot2. Faceting is performed by adding a facet() function to 
  the end of our plotting statement.
       
    There are two types of plot layers:
    
    1-Layering additional plots that use the same variables and input data as 
      the original plot
      
    2-Layering of additional plots that use different but complementary data 
      to the original plot
      
    to recreate our previous boxplot example comparing the highway fuel 
    efficiency across manufacturers, add our data points using the 
    geom_point() function:
      
    #import dataset into ggplot2
      
    plt <- ggplot(mpg,aes(x=manufacturer, y=hwy))
    
    #add boxplot +   
    plt + geom_boxplot() + 
      
    #rotate x-axis labels 45 degrees +
    theme(axis.text.x=element_text(angle=45,hjust=1))+
    
    #overlay scatter plot on top
    geom_point()
    
  This new information can provide the reader better context when comparing 
  two manufacturers with similarly shaped boxplots.
    
  There may be instances when we would want to add additional plotting layers 
  with new and complementary data. The mapping argument functions exactly the 
  same as our ggplot() function,
  
  #create summary table
  mpg_summary <- mpg %>% group_by(class) %>% summarize(Mean_Engine=mean(displ), .groups = 'keep') 
  
  #import dataset into ggplot2
  plt <- ggplot(mpg_summary,aes(x=class,y=Mean_Engine)) 
  
  #add scatter plot
  plt + geom_point(size=4) + labs(x="Vehicle Class",y="Mean Engine Size") 
  
  Although this plot sufficiently visualizes the means, its critical that we 
  provide context around the standard deviation of the engine size for each 
  vehicle class. If we compute the standard deviations in our dplyr
  summarize() function, we can layer the upper and lower standard deviation 
  boundaries to our visualization using the geom_errorbar() function:
    
    
  mpg_summary <- mpg %>% group_by(class) %>% summarize(Mean_Engine=mean(displ),SD_Engine=sd(displ), .groups = 'keep') 
  
  #import dataset into ggplot2
  plt <- ggplot(mpg_summary,aes(x=class,y=Mean_Engine))
  
  #add scatter plot with labels
  plt+ geom_point (size=4) +labs (x="Vehicle_Class", y="Mean Engine Size")+ 
    
  #overlay with error bars
  geom_errorbar(aes(ymin=Mean_Engine-SD_Engine, ymax=Mean_Engine+SD_Engine))
  
  #convert to long format
  mpg_long <- mpg %>% gather(key="MPG_Type",value="Rating",c(cty,hwy)) 
  head(mpg_long)
  
  #import dataset into ggplot2
  plt <- ggplot(mpg_long,aes(x=manufacturer,y=Rating,color=MPG_Type)) 
  
  #add boxplot with labels rotated 45 degrees
  plt + geom_boxplot() + theme(axis.text.x=element_text(angle=45,hjust=1)) 
  
  This boxplot is optimal for comparing the city versus highway fuel efficiency
  for each manufacturer, but it is more difficult to compare all of the city 
  fuel efficiency across manufacturers.
  
  One solution using the facet_wrap() function.
  facet_wrap() function has many optional variables to tweak the direction and 
  type of faceting. The facets argument expects a list of grouping variables to 
  facet by using the vars() function.
  
  To facet our previous example by the fuel-efficiency type, our R code could be
  
  #import dataset into ggplot2
  plt <- ggplot(mpg_long,aes(x=manufacturer,y=Rating,color=MPG_Type))
  
  #create multiple boxplots, one for each MPG type
  plt + geom_boxplot() + facet_wrap(vars(MPG_Type)) +
  
    
  #rotate x-axis labels
  theme(axis.text.x=element_text(angle=45,hjust=1),legend.position = "none") + 
  xlab("Manufacturer")
    
  
  
  

  #15.4.1 Identifying Statistical Test Types ----
  
  Identifying Statistical Test Types
  
    #15.4.2 Identify Different Data Types ----
  
  Identify Different Data Types;
  
  Cagegorical Data:
  
        dichotomous== Data that is one of two possible values
        
        ordinal   == Categorical data that has directions, but the distance between
                      the values is ambigous
                      
        nominal   == Data used as labels and names
        
  Numerical Data:
  
        continuous== Numerical data that can be infinetely precise
        
        interval  == Numerical data spaced evenly out on a scale
  
  
  
  
    
  
  
  
  
  
        #15.4.3 Dive Into Distributions ----
    Dive Into Distributions
        If we make incorrect assumptions about the distribution of our data, 
        our statistical results could be meaningless. In general, most basic 
        statistical tests assume that each numerical metric follows an 
        approximate normal distribution.
        
    What Is Normal Distribution?
    Normal distribution, or normality, is commonly referred to as 
    "the bell curve," and describes a dataset where values farther from its 
    mean occur less frequently than values closer to its mean.
    
    When numerical data is considered to be normally distributed, the probability
    of any data point follows the 68-95-99.7 rule, stating that 68.27%, 95.45%, 
    and 99.73% (effectively 100%) of the values lie within one, two, and 
    three standard deviations of the mean, respectively.
        
        
        
            #15.4.4 Test for Normality ----
    
    Test for Normality
      You can test for normality during data analysis by performing a qualitative
      test or a quantitative test.
    
    Qualitative Test for Normality:
      The qualitative test for normality is a visual assessment of the distribution 
      of data, which looks for the characteristic bell curve shape across the 
      distribution. In R, we would use ggplot2 to plot the distribution using
      the geom_density() function.
      
      ****************************************************************
        
        For example, if we want to test the distribution of vehicle weights 
        from the built-in mtcars dataset, our R code would be as follows:
        
          #visualize distribution using density plot
        ggplot(mtcars,aes(x=wt)) + geom_density() 
    
              #15.4.5 Understand Skew ----
        Understand Skew
        
        
        Relatively smaller sample sizes, our data distributions are often 
        asymmetrical. Compared to the normal distribution, where each tail of
        the distribution (on either side of the mean μ) mirrors one another, 
        the asymmetrical distribution has one distribution tail that is longer
        than the other. This asymmetrical distribution is commonly referred to 
        as a skewed distribution and 
        
        there are two types—left skew and rightskew.
        
        LEFT SKEW A data distribution is considered to be left skewed, or 
        negative skewed, if the left tail is longer than the right,
        
        RIGHT SKEW A data distribution is considered to be right skewed, or 
        positive skewed, if the right tail is longer than the left
        
        
        As with most problems in data analytics, we must approach skewness on 
        a case-by-case basis. Depending on the severity of the skewness and 
        the size of the dataset, there are multiple means of dealing 
        (or not dealing) with skewness.
        
        If our data is smaller, or the skewness does impact the overall shape 
        or our distribution,
        
          if possible add more data points to alleviate the effect of skew.
        
          Resample or regenerate daa f we think that the data might not be 
          representative of the original conditions or dataset
          
          By using a log-transformation, the effects of extreme values are 
          reduced, and this transformation can help make each distribution 
          tail more symmetrical.
          
  #15.5.1  Practice Hypothesis Testing ----
          Practice Hypothesis Testing
         
     One of the largest and most critical concepts in statistics is hypothesis
     testing.We use statistical hypothesis testing to determine the probability 
     of an event (or set of observations) under particular assumptions. 
     In other words, we use statistical hypothesis testing to determine which 
     of our hypotheses are most likely to be true. There are two types of 
     statistical hypothesis:      
       
          The null hypothesis is also known as H0 and is generally the hypothesis 
         that can be explained by random chance.
     
         The alternate hypothesis is also known as Ha and is generally the 
         hypothesis that is influenced by non-random events.
              
      HYPOTHESIS TESTING IN 5 STEPS
      
      1-Generate a null hypothesis, 
      2-Identify a statistical analysis to assess the truth of the null hypothesis
      3-Compute the p value using statistical analysis.
      4-Compare p-value to the significance levels.default
      5-Reject (or fail to reject) the null hypothesis and generate the conculusion.
      
          
          
          
          
          
          
          
    #15.5.2
      
  #15.6.1 Sample Versus Population Dataset ----
      
      Sample Versus Population Dataset
      
  Population Dataset == an ideal dataset that contains measurements and results 
  from every possible outcome, condition, or consideration.
  
  Sample Dataset == subset of the population dataset, where not all elements
  of a study or experiment are collected or measured.
  
  To produce a sample dataset that has a similar distribution to the population 
  data, most statisticians suggest using random sampling. Random sampling is a 
  technique in data science in which every subject or data point has an equal 
  chance of being included in the sample. This technique increases the 
  likelihood that even a small sample size will include individuals from each 
  "group" within the population.
  
  Built-in sample()function in R, or sample_n() function from dplyr, the 
  resulting sample distributions should be similar to the input population data. 
  
    Using sample_n() function requires 2 argument;
    
    1-tbl ---- the name of the input table. Optionally, we can use a dplyr 
      pipe (%>%) to provide the data frame object directly
    2-size---- the number of rows to return()
    
    *****************************************************
  To practice generating samples using random sampling;
    
    visualize the distribution of driven miles for our entire population 
    dataset, we can use the geom_density()function from ggplot2:
      
    #import used car dataset
population_table <- read.csv('used_car_data.csv',check.names = F,stringsAsFactors = F)
    
    #import dataset into ggplot2
plt<- ggplot(population_table, aes(x=log10(Miles_Driven)))   

    #visualize distribution using density plot
plt + geom_density()

  #######we'll create a sample dataset using dplyr's sample_n()function

    #randomly sample 50 data points
sample_table<- population_table%>% sample_n(50)

    #import dataset into ggplot2
plt<- ggplot(sample_table, aes(x=log10(Miles_Driven)))

    #visualize distribution using density plot
plt + geom_density()


      
    
    
  
  
  
  
  
  
  
  
    #15.6.2  Use the One-Sample t-Test ----
Use the One-Sample t-Test

In statistics, we use a t-test to compare the mean of one dataset to another 
under a few assumptions.
There are two main forms of the t-test that we use: 
  
  the one-sample t-test 

      The one-sample t-test is used to determine whether there is a statistical 
      difference between the means of a sample dataset and a hypothesized,
      potential population dataset
      
      In R,we can implement a one-sample t-test using the built-in stats package 
      t.test()function
      
      **********************************************************
        if we want to test if the miles driven from our previous sample dataset 
        is statistically different from the miles driven in our population data, 
        we would use our t.test()function as follows:
          
          #compare sample versus population means
      
        t.test(log10(sample_table$Miles_Driven), mu=mean(log10(population_table$Miles_Driven)))
     
         **********************************************************
      the two-sample t-test ==wo-sample t-Test determines whether the means of 
      two samples are statistically different. In other words,
      
      ****************************************************
    As practice, lets test whether the mean miles driven of two samples 
    from our used car dataset are statistically different.
        
    #generate 50 randomly sampled data points
    sample_table <- population_table %>% sample_n(50)
    
    #generate another 50 randomly sampled data points
    sample_table2 <- population_table %>% sample_n(50)
    
    #compare means of two samples
    t.test(log10(sample_table$Miles_Driven),log10(sample_table2$Miles_Driven))


      #15.6.4  Use the Two-Sample t-Test to Compare Samples ----
    Use the Two-Sample t-Test to Compare Samples
    The two-sample t-test will be used to compare two samples from a single 
    population dataset. However, two-sample t-tests are flexible and can be 
    used for another purpose: to compare two samples, each from a different 
    population. This is known as a pair t-test
    
    When it comes to implementing a paired t-test in R, well use the 
    t.test() function.
    
      t.test(x,y=NULL, alternative = alternative = c("two.sided", "less", "greater"),
                      mu = 0, paired = FALSE, var.equal = FALSE,
                      conf.level = 0.95, ...)
    
      x=(non-empty) numeric vector of data values.
      y=an optional (non-empty) numeric vector of data values.
      alternative= a character string specifying the alternative hypothesis
      mu= a number indicating the true value of the mean
  
  ****************************************************************
        To practice calculating a paired t-test in R, download the modified 
        mpg dataset (Links to an external site.) data file contains a modified 
        version of Rs built-in mpg dataset, where each 1999 vehicle was paired 
        with a corresponding 2008 vehicle.
        
        #import dataset
        mpg_data <- read.csv('mpg_modified.csv')
        
        #select only data points where the year is 1999
        mpg_1999 <- mpg_data %>% filter(year==1999)
        
        #select only data points where the year is 2008
        mpg_2008 <- mpg_data %>% filter(year==2008)
        
        Now that we have our paired datasets, we can use a paired t-test to 
        determine if there is a statistical difference in overall highway fuel 
        efficiency between vehicles manufactured in 1999 versus 2008. In other 
        words, we are testing our null hypothesis—that the overall difference 
        is zero. Using our t.test() function in R, our code would be as follows:
        
          #compare the mean difference between two samples
          t.test(mpg_1999$hwy,mpg_2008$hwy,paired = T)

    

        
        #15.6.5  Use the ANOVA Test ----
        Use the ANOVA Test
        
  #15.7.1  The Correlation Conundrum  ----
        
  Correlation analysis is a statistical technique that identifies how 
  strongly (or weakly) two variables are related.
    
  Correlation is quantified by calculating a correlation coefficient, and 
  the most common correlation coefficient is the Pearson correlation coefficient. 
  The Pearson correlation coefficient is denoted as "r" in mathematics and is 
  used to quantify a linear relationship between two numeric variables. 
  The Pearson correlation coefficient ranges between -1 and 1, depending 
  on the direction of the linear relationship.
    
  ideal positive correcation, When two variables are positively correlated, 
  they move in the same direction  and ideal positive correcation r=1
  
  ideal negative correlation where r = -1. When two variables are negatively 
  correlated, they move in opposite directions.
    
  no correlation where r ≈ 0. When two variables are not correlated, their 
  values are completely independent between one another. 
    
  Absolute Value of r	        Strength of Correlation
  r < 0.3	                    None or very weak
  0.3 ≤ r < 0.5	              Weak
  0.5 ≤ r < 0.7	              Moderate
  r ≥ 0.7	                    Strong
  
  In R, we can use our geom_point() plotting function combined with the 
  cor() function to quantify the correlation between variables.
    
  **********************************************************
    
    well use the mtcars dataset as example;In the mtcars dataset, 
    there are a number of numeric columns that we can use to test for 
    correlation such as mpg, disp, hp, drat, wt, and qsec. For our example, 
    well test whether or not horsepower (hp) is correlated with 
    quarter-mile race time (qsec).
  
  head(mtcars)
  #import dataset into ggplot2
  plt <- ggplot(mtcars,aes(x=hp,y=qsec))
  
  #create scatter plot
  plt + geom_point()
    
  Looking at out plot ehicle horsepower increases, vehicle quarter-mile time 
  decreases.
  
  #we'll use our cor() function to quantify the strength of the correlation 
  #between our two variables:
  cor(mtcars$hp,mtcars$qsec)
  
  we have determined that the r-value between horsepower and quarter-mile time 
  is -0.71, which is a strong negative correlation.
  
  For another example, lets reuse our used_cars dataset:
  #read in dataset
  used_cars <- read.csv('used_car_data.csv',stringsAsFactors = F)
  head(used_cars)
  
  For this example, well test whether or not vehicle miles driven and selling 
  price are correlated. Once again, well plot our two variables using the 
  geom_point() function:
  
    #import dataset into ggplot2
    plt <- ggplot(used_cars,aes(x=Miles_Driven,y=Selling_Price))
  
  #create a scatter plot
  plt + geom_point()
  
  our scatter plot did not help us determine whether or not our two variables 
  are correlated. However, lets see what happens if we calculate the Pearson 
  correlation coefficient using the cor() function:
    
    #calculate correlation coefficient
    cor(used_cars$Miles_Driven,used_cars$Selling_Price)
  
  0.02918709
    
  Our calculated r-value is 0.02, which means that there is a negligible 
  correlation between miles driven and selling price in this dataset.
  
  Instead of computing each pairwise correlation, we can use the cor() function
  to produce a correlation matrix. A correlation matrix is a lookup table 
  where the variable names of a data frame are stored as rows and columns, 
  and the intersection of each variable is the corresponding Pearson 
  correlation coefficient. We can use the cor() function to produce a 
  correlation matrix by providing a matrix of numeric vectors.
  
  
  if we want to produce a correlation matrix for our used_cars dataset, 
  we would first need to select our numeric columns from our data frame and 
  convert to a matrix. Then we can provide our numeric matrix to the 
  cor() function as follows:
  
    #convert data frame into numeric matrix
    
    used_matrix <- as.matrix(used_cars[,c("Selling_Price","Present_Price","Miles_Driven")])
    cor(used_matrix)
    
    Selling_Price    1.00000000     0.8789825   0.02918709
    Present_Price    0.87898255     1.0000000   0.20364703
    Miles_Driven     0.02918709     0.2036470   1.00000000
        
    #5.7.2  Return to Linear Regression ----
        Return to Linear Regression
    
    
    linear regression is a statistical model that is used to predict a 
    continuous dependent variable based on one or more independent variables 
    fitted to the equation of a line.
    
    equation of a line is written as 
    
      'y = mx + b'
      
      y == Dependent variable
      m == Slope
      x == Independent variable
      b == y-intercept
      
      'Simple linear regression' builds a linear regression model with one 
      independent variable.
      
      'Multiple linear regression' builds a linear regression model with two 
      or more independent variables.
      
      To quantify how well our linear model can be used to predict future 
      observations, our linear regression functions will calculate an 
      r-squared value. The' r-squared (r2) value'is also known as the 
      coefficient of determination and represents how well the regression 
      model approximates real-world data points.
      
      By combining the p-value of our hypothesis test with the r-squared value,
      the linear regression model becomes a powerful statistics tool that both 
      quantifies a relationship between variables and provides a meaningful 
      model to be used in any decision-making process.
      
      In R, well build our linear models using the built-in lm()function
      
      lm() function only requires us to provide two arguments:
        
        1-'formula'==  is the same R statement that we use for the aov() function. 
        The formula statement tells R how to interpret the different variables 
        and factors. With simple linear regression, well use the formula 
        'Y ~ A where Y is the column name of the dependent variable', and 
        'A is the column name of the independent variable.'
        
        2-'data' ==is the name of our input data frame. The data frame should 
        contain columns for each variable.
        
        ************************************************
          lets revisit our correlation example using the mtcars dataset. 
          Using our simple linear regression model, well test whether or not 
          quarter-mile race time (qsec) can be predicted using a linear model 
          and horsepower (hp).
          
          linear regression model, our R statement would be as follows:
            
            #create linear model
            lm(qsec ~ hp,mtcars) 
          ______________________________________________
          Call:
            lm(formula = qsec ~ hp, data = mtcars)
          
          Coefficients:
            (Intercept)           hp  
            20.55635            -0.01846
            
            qsec = -0.02hp + 20.56
          _______________________________________________
            
    To determine our p-value and our r-squared value for a simple linear 
    regression model, well use the summary() function:
      
      #summarize linear model
      summary(lm(qsec~hp,mtcars)) 
          
    Coefficients:
      Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 20.556354   0.542424  37.897  < 2e-16 ***
      hp          -0.018458   0.003359  -5.495 5.77e-06 ***
      ---
      Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    
    Residual standard error: 1.282 on 30 degrees of freedom
    Multiple R-squared:  0.5016,	Adjusted R-squared:  0.485 
    F-statistic: 30.19 on 1 and 30 DF,  p-value: 5.766e-06
    
    From our linear regression model, the r-squared value is 0.50, which means 
    that roughly 50% of all quarter mile time predictions will be correct when 
    using this linear model
    
    In addition, the p-value of our linear regression analysis is 5.77 x 10-6, 
    which is much smaller than our assumed significance level of 0.05%. 
    Therefore, we can state that there is sufficient evidence to reject our 
    null hypothesis, which means that the slope of our linear model is not zero.
    
    Once we have calculated our linear regression model, we can visualize the 
    fitted line against our dataset using ggplot2.
    
    #create linear model
    model <- lm(qsec ~ hp,mtcars)
    yvals <- model$coefficients['hp']*mtcars$hp +
    
    #determine y-axis values from linear model  
    model$coefficients['(Intercept)']
    
    Once we have calculated our line plot data points, we can plot the linear 
    model over our scatter plot:
    
    #import dataset into ggplot2
    plt <- ggplot(mtcars,aes(x=hp,y=qsec))
    
    #plot scatter and linear model
    plt + geom_point() + geom_line(aes(y=yvals), color = "red")
    
    **********************************************************
      
  # 15.7.3 Perform Multiple Linear Regression ----
    
    Multiple linear regression is a statistical model that extends the scope 
    and flexibility of a simple linear regression model.
    
    linear regression equation becomes 
    
    y = m1x1 + m2x2 + … + mnxn + b, 
    
    for all independent x variables and their m coefficients.
    
    To practice multiple linear regression, lets revisit our mtcars dataset. 
    From our last example, we determined that quarter-mile time was not 
    adequately predicted from just horsepower. To better predict the 
    quarter-mile time (qsec) dependent variable, we can add other variables 
    of interest such as fuel efficiency (mpg), engine size (disp), rear axle 
    ratio (drat), vehicle weight (wt), and horsepower (hp) as independent 
    variables to our multiple linear regression model.
    
    #generate multiple linear regression model
    lm(qsec ~ mpg + disp + drat + wt + hp,data=mtcars)
    

    In R, our multiple linear regression statement is as follows:
      Call:
      lm(formula = qsec ~ mpg + disp + drat + wt + hp, data = mtcars)
    _________________________________________________
    Coefficients:
    (Intercept)      mpg         disp         drat           wt           hp  
    16.541651     0.108579    -0.008076    -0.578953     1.792793    -0.018383 
    
    
    #generate summary statistics
    summary(lm(qsec ~ mpg + disp + drat + wt + hp,data=mtcars))
    
  #15.8.1 Category Complexities ----
    Category Complexities
    
    
      